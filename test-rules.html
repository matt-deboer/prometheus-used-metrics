<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>Prometheus Time Series Collection and Processing Server</title>
    <link rel="shortcut icon" href="/static/img/favicon.ico?v=3afb3fffa3a29c3de865e1172fb740442e9d0133">
    <script src="/static/vendor/js/jquery.min.js?v=3afb3fffa3a29c3de865e1172fb740442e9d0133"></script>
    <script src="/static/vendor/bootstrap-3.3.1/js/bootstrap.min.js?v=3afb3fffa3a29c3de865e1172fb740442e9d0133"></script>

    <link type="text/css" rel="stylesheet" href="/static/vendor/bootstrap-3.3.1/css/bootstrap.min.css?v=3afb3fffa3a29c3de865e1172fb740442e9d0133">
    <link type="text/css" rel="stylesheet" href="/static/css/prometheus.css?v=3afb3fffa3a29c3de865e1172fb740442e9d0133">

    <script>
      var PATH_PREFIX = "";
      var BUILD_VERSION = "3afb3fffa3a29c3de865e1172fb740442e9d0133";
      $(function () {
        $('[data-toggle="tooltip"]').tooltip()
      })
    </script>

    
  </head>

  <body>
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container-fluid">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/">Prometheus</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav navbar-left">
            
            
            <li><a href="/alerts">Alerts</a></li>
            <li><a href="/graph">Graph</a></li>
            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Status <span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="/status">Runtime &amp; Build Information</a></li>
                <li><a href="/flags">Command-Line Flags</a></li>
                <li><a href="/config">Configuration</a></li>
                <li><a href="/rules">Rules</a></li>
                <li><a href="/targets">Targets</a></li>
              </ul>
            </li>
            <li>
              <a href="https://prometheus.io" target="_blank">Help</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    
  <div class="container-fluid">
    <h2 id="rules">Rules</h2>
    <pre>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22tractorbeam_elasticsearch_errors%22%7D&g0.tab=0">tractorbeam_elasticsearch_errors</a>
  IF <a href="/graph?g0.expr=rate%28tractorbeam_tractorbeam_elasticsearch_error%5B1m%5D%29+%3E+0&g0.tab=0">rate(tractorbeam_tractorbeam_elasticsearch_error[1m]) &gt; 0</a>
  FOR 1m
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;ContributorStats Tractorbeam has ElasticSearch failures.&#34;, summary=&#34;ContributorStats Tractorbeam has ElasticSearch failures&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22tractorbeam_elasticsearch_no_activity%22%7D&g0.tab=0">tractorbeam_elasticsearch_no_activity</a>
  IF <a href="/graph?g0.expr=absent%28rate%28tractorbeam_tractorbeam_es_document_saved%5B5m%5D%29%29&g0.tab=0">absent(rate(tractorbeam_tractorbeam_es_document_saved[5m]))</a>
  FOR 1m
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;ContributorStats Tractorbeam: no documents saved to ElasticSearch.&#34;, summary=&#34;ContributorStats Tractorbeam: no documents saved to ElasticSearch&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_apps_elasticsearch_contrib_alias_and_trim_failing%22%7D&g0.tab=0">jaguar_apps_elasticsearch_contrib_alias_and_trim_failing</a>
  IF <a href="/graph?g0.expr=sum%28changes%28chronos_jobs_run_success%7Bchronos_job%3D%22jaguar_apps_elasticsearch_contrib_alias_and_trim%22%7D%5B1d%5D%29%29+%3D%3D+0&g0.tab=0">sum(changes(chronos_jobs_run_success{chronos_job=&#34;jaguar_apps_elasticsearch_contrib_alias_and_trim&#34;}[1d])) == 0</a>
  FOR 1h
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;jaguar_apps_elasticsearch_contrib_alias_and_trim hasn&#39;t succeeded in over a day&#34;, summary=&#34;jaguar_apps_elasticsearch_contrib_alias_and_trim hasn&#39;t succeeded in over a day&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_apps_trim_daily_contrib_views_failing%22%7D&g0.tab=0">jaguar_apps_trim_daily_contrib_views_failing</a>
  IF <a href="/graph?g0.expr=sum%28changes%28chronos_jobs_run_success%7Bchronos_job%3D%22jaguar_apps_trim_daily_contrib_views%22%7D%5B1d%5D%29%29+%3D%3D+0&g0.tab=0">sum(changes(chronos_jobs_run_success{chronos_job=&#34;jaguar_apps_trim_daily_contrib_views&#34;}[1d])) == 0</a>
  FOR 1h
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;jaguar_apps_trim_daily_contrib_views hasn&#39;t succeeded in over a day&#34;, summary=&#34;jaguar_apps_trim_daily_contrib_views hasn&#39;t succeeded in over a day&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_contributor_stats_elasticsearch_snapshot_failing%22%7D&g0.tab=0">jaguar_contributor_stats_elasticsearch_snapshot_failing</a>
  IF <a href="/graph?g0.expr=sum%28changes%28chronos_jobs_run_success%7Bchronos_job%3D%22jaguar_contributor_stats_elasticsearch_snapshot%22%7D%5B1d%5D%29%29+%3D%3D+0&g0.tab=0">sum(changes(chronos_jobs_run_success{chronos_job=&#34;jaguar_contributor_stats_elasticsearch_snapshot&#34;}[1d])) == 0</a>
  FOR 1h
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;jaguar_contributor_stats_elasticsearch_snapshot hasn&#39;t succeeded in over a day&#34;, summary=&#34;jaguar_contributor_stats_elasticsearch_snapshot hasn&#39;t succeeded in over a day&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_contributor_stats_daily_was_not_successful_in_26_hours%22%7D&g0.tab=0">jaguar_contributor_stats_daily_was_not_successful_in_26_hours</a>
  IF <a href="/graph?g0.expr=max%28max_over_time%28spark_JaguarContributorStatsDaily_driver_contributor_stats_daily_success_total_count%5B26h%5D%29%29+%3D%3D+0&g0.tab=0">max(max_over_time(spark_JaguarContributorStatsDaily_driver_contributor_stats_daily_success_total_count[26h])) == 0</a>
  FOR 1h
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;ContributorStatsDaily job was not successful in 26 hours&#34;, summary=&#34;ContributorStatsDaily job was not successful in 26 hours&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_contributor_stats_monthly_was_not_successful_in_26_hours%22%7D&g0.tab=0">jaguar_contributor_stats_monthly_was_not_successful_in_26_hours</a>
  IF <a href="/graph?g0.expr=max%28max_over_time%28spark_SparkContributorStatsMonthly_driver_contributor_stats_monthly_success_total_count%5B26h%5D%29%29+%3D%3D+0&g0.tab=0">max(max_over_time(spark_SparkContributorStatsMonthly_driver_contributor_stats_monthly_success_total_count[26h])) == 0</a>
  FOR 1h
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;ContributorStatsMonthly job was not successful in 26 hours&#34;, summary=&#34;ContributorStatsMonthly job was not successful in 26 hours&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_contributor_stats_royalties_was_not_successful_in_26_hours%22%7D&g0.tab=0">jaguar_contributor_stats_royalties_was_not_successful_in_26_hours</a>
  IF <a href="/graph?g0.expr=max%28max_over_time%28spark_SparkContributorStatsRoyalties_driver_contributor_stats_royalties_success_total_count%5B26h%5D%29%29+%3D%3D+0&g0.tab=0">max(max_over_time(spark_SparkContributorStatsRoyalties_driver_contributor_stats_royalties_success_total_count[26h])) == 0</a>
  FOR 1h
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;ContributorStatsRoyalties job was not successful in 26 hours&#34;, summary=&#34;ContributorStatsRoyalties job was not successful in 26 hours&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22chrome_extension_image_cache_refresh_failing%22%7D&g0.tab=0">chrome_extension_image_cache_refresh_failing</a>
  IF <a href="/graph?g0.expr=sum%28changes%28chronos_jobs_run_success%7Bchronos_job%3D%22chrome-extension-image-cache-refresh%22%7D%5B1w%5D%29%29+%3D%3D+0&g0.tab=0">sum(changes(chronos_jobs_run_success{chronos_job=&#34;chrome-extension-image-cache-refresh&#34;}[1w])) == 0</a>
  FOR 1h
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;The chrome-extension-image-cache-refresh Chronos job hasn&#39;t had a succesful run in over 7 days.&#34;, summary=&#34;Chrome extension image cache refresh is failing&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22chrome_ext_bandit_not_processed%22%7D&g0.tab=0">chrome_ext_bandit_not_processed</a>
  IF <a href="/graph?g0.expr=sum%28changes%28chronos_jobs_run_success%7Bchronos_job%3D%22chrome-ext-bandit%22%7D%5B10m%5D%29%29+%3D%3D+0&g0.tab=0">sum(changes(chronos_jobs_run_success{chronos_job=&#34;chrome-ext-bandit&#34;}[10m])) == 0</a>
  FOR 20m
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;The chrome-ext-bandit Chronos job hasn&#39;t had a successful run in over 20 minutes.&#34;, summary=&#34;chrome-ext-bandit is failing&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22chrome_extension_becaptioner_failing%22%7D&g0.tab=0">chrome_extension_becaptioner_failing</a>
  IF <a href="/graph?g0.expr=sum%28changes%28chronos_jobs_run_success%7Bchronos_job%3D%22chrome-extension-becaptioner%22%7D%5B1h%5D%29%29+%3D%3D+0&g0.tab=0">sum(changes(chronos_jobs_run_success{chronos_job=&#34;chrome-extension-becaptioner&#34;}[1h])) == 0</a>
  FOR 1m
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;The chrome-extension-becaptioner Chronos job hasn&#39;t had a succesful run in over 60 minutes.&#34;, summary=&#34;Chrome extension becaptioner is failing&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22embed_asset_changes_kicker_failing%22%7D&g0.tab=0">embed_asset_changes_kicker_failing</a>
  IF <a href="/graph?g0.expr=sum%28changes%28chronos_jobs_run_success%7Bchronos_job%3D%22embed-asset-changes-kicker%22%7D%5B1h%5D%29%29+%3D%3D+0&g0.tab=0">sum(changes(chronos_jobs_run_success{chronos_job=&#34;embed-asset-changes-kicker&#34;}[1h])) == 0</a>
  FOR 1m
  LABELS {project=&#34;embed&#34;}
  ANNOTATIONS {description=&#34;The embed-asset-changes-kicker Chronos job hasn&#39;t had a successful run in over 60 minutes.&#34;, summary=&#34;Embed asset-changes kicker is failing&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22embed_becaptioner_failing%22%7D&g0.tab=0">embed_becaptioner_failing</a>
  IF <a href="/graph?g0.expr=sum%28changes%28chronos_jobs_run_success%7Bchronos_job%3D%22embed-becaptioner%22%7D%5B1h%5D%29%29+%3D%3D+0&g0.tab=0">sum(changes(chronos_jobs_run_success{chronos_job=&#34;embed-becaptioner&#34;}[1h])) == 0</a>
  FOR 1m
  LABELS {project=&#34;embed&#34;}
  ANNOTATIONS {description=&#34;The embed-becaptioner Chronos job hasn&#39;t had a succesful run in over 60 minutes.&#34;, summary=&#34;Embed becaptioner is failing&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22embed_becaptioner_events_failing%22%7D&g0.tab=0">embed_becaptioner_events_failing</a>
  IF <a href="/graph?g0.expr=sum%28changes%28chronos_jobs_run_success%7Bchronos_job%3D%22embed-becaptioner-events%22%7D%5B1h%5D%29%29+%3D%3D+0&g0.tab=0">sum(changes(chronos_jobs_run_success{chronos_job=&#34;embed-becaptioner-events&#34;}[1h])) == 0</a>
  FOR 1m
  LABELS {project=&#34;embed&#34;}
  ANNOTATIONS {description=&#34;The embed-becaptioner-events Chronos job hasn&#39;t had a succesful run in over 60 minutes.&#34;, summary=&#34;Embed becaptioner for events is failing&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22prometheus_failure_alert%22%7D&g0.tab=0">prometheus_failure_alert</a>
  IF <a href="/graph?g0.expr=sum%28up%7Bjob%3D%22prometheus%22%7D%29+%2F+sum%28marathon_app_instances%7Bapp%3D~%22%2Fsystem%2Fprometheus-ha-aws%7C%2Fsystem%2Fprometheus-ha%22%7D%29+%3C+0.35&g0.tab=0">sum(up{job=&#34;prometheus&#34;}) / sum(marathon_app_instances{app=~&#34;/system/prometheus-ha-aws|/system/prometheus-ha&#34;}) &lt; 0.35</a>
  FOR 10m
  LABELS {owner=&#34;system&#34;, severity=&#34;warning&#34;}
  ANNOTATIONS {description=&#34;Prometheus resiliency is too low :flappy:&#34;, summary=&#34;The running prometheus instance rate is less than 2/3&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_flume_signals_no_events_written_to_s3%22%7D&g0.tab=0">jaguar_flume_signals_no_events_written_to_s3</a>
  IF <a href="/graph?g0.expr=rate%28flume_sink_s3_EventDrainSuccessCount%7Bmarathon_app%3D~%22%2Fjaguar%2Fflume%2F.%2B-signals%22%7D%5B10m%5D%29+%3D%3D+0&g0.tab=0">rate(flume_sink_s3_EventDrainSuccessCount{marathon_app=~&#34;/jaguar/flume/.+-signals&#34;}[10m]) == 0</a>
  FOR 1m
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;No signals written to S3 in the last 10 minutes.&#34;, summary=&#34;No signals written to S3&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_metadata_logstash_rate%22%7D&g0.tab=0">jaguar_metadata_logstash_rate</a>
  IF <a href="/graph?g0.expr=logstash_jaguar_assetMetadata_rate_1m+%3C+50&g0.tab=0">logstash_jaguar_assetMetadata_rate_1m &lt; 50</a>
  FOR 1h
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;Metadata Logstash did not push enough assets to elastic search&#34;, summary=&#34;Metadata Logstash did not push enough assets to elastic search&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_getty_parquet_converter_not_processed%22%7D&g0.tab=0">jaguar_getty_parquet_converter_not_processed</a>
  IF <a href="/graph?g0.expr=max%28max_over_time%28spark_JaguarGettySignalsParquetConverter_driver_gi_json_parquet_processed_total_count%5B26h%5D%29%29+%3D%3D+0&g0.tab=0">max(max_over_time(spark_JaguarGettySignalsParquetConverter_driver_gi_json_parquet_processed_total_count[26h])) == 0</a>
  FOR 1h
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;Json to Parquet files were not processed.&#34;, summary=&#34;Json to Parquet files were not processed&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_istock_parquet_converter_not_processed%22%7D&g0.tab=0">jaguar_istock_parquet_converter_not_processed</a>
  IF <a href="/graph?g0.expr=max%28max_over_time%28spark_JaguariStockSignalsParquetConverter_driver_is_json_parquet_processed_total_count%5B26h%5D%29%29+%3D%3D+0&g0.tab=0">max(max_over_time(spark_JaguariStockSignalsParquetConverter_driver_is_json_parquet_processed_total_count[26h])) == 0</a>
  FOR 1h
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;Json to Parquet files were not processed.&#34;, summary=&#34;Json to Parquet files were not processed&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_popular_events_not_processed%22%7D&g0.tab=0">jaguar_popular_events_not_processed</a>
  IF <a href="/graph?g0.expr=max%28max_over_time%28spark_JaguarPopularEvents_driver_gi_popularevents_success_total_count%5B3h%5D%29%29+%3D%3D+0&g0.tab=0">max(max_over_time(spark_JaguarPopularEvents_driver_gi_popularevents_success_total_count[3h])) == 0</a>
  FOR 1h
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;Popular events were not processed.&#34;, summary=&#34;Popular events were not processed&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_search_trends_errors%22%7D&g0.tab=0">jaguar_search_trends_errors</a>
  IF <a href="/graph?g0.expr=rate%28spark_SearchTrends_driver_job_failed_count_total_count%5B15m%5D%29+%21%3D+0&g0.tab=0">rate(spark_SearchTrends_driver_job_failed_count_total_count[15m]) != 0</a>
  FOR 20m
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;SearchTrends has had errors within the last 20 minutes&#34;, summary=&#34;SearchTrends has had errors&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_spark_eloqua_not_processed%22%7D&g0.tab=0">jaguar_spark_eloqua_not_processed</a>
  IF <a href="/graph?g0.expr=sum%28max_over_time%28spark_Eloqua_driver_eloqua_success_total_count%5B26h%5D%29%29+%3D%3D+0&g0.tab=0">sum(max_over_time(spark_Eloqua_driver_eloqua_success_total_count[26h])) == 0</a>
  FOR 6h
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;Spark Eloqua job has not run in 24 hours.&#34;, summary=&#34;Spark Eloqua job has not run in 24 hours.&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_asset_view_trails_not_running%22%7D&g0.tab=0">jaguar_asset_view_trails_not_running</a>
  IF <a href="/graph?g0.expr=absent%28rate%28spark_GettyAssetViewTrails_driver_StreamingMetrics_streaming_totalCompletedBatches%5B5m%5D%29%29+%3D%3D+1&g0.tab=0">absent(rate(spark_GettyAssetViewTrails_driver_StreamingMetrics_streaming_totalCompletedBatches[5m])) == 1</a>
  FOR 1m
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;AssetViewTrails for Getty has not completed any batches within the last minute&#34;, summary=&#34;Asset view trails not running&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_asset_view_trails_not_writing_to_redis%22%7D&g0.tab=0">jaguar_asset_view_trails_not_writing_to_redis</a>
  IF <a href="/graph?g0.expr=rate%28spark_GettyAssetViewTrails_driver_redis_writes_total_count%5B1m%5D%29+%3D%3D+0&g0.tab=0">rate(spark_GettyAssetViewTrails_driver_redis_writes_total_count[1m]) == 0</a>
  FOR 3m
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;AssetViewTrails for Getty has not written to Redis within the last minute&#34;, summary=&#34;Asset view trails not writing to redis&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_metadata_generator_not_processed%22%7D&g0.tab=0">jaguar_metadata_generator_not_processed</a>
  IF <a href="/graph?g0.expr=max%28max_over_time%28spark_AssetMetadataGenerator_driver_gi_metadatagenerator_success_total_count%5B3h%5D%29%29+%3D%3D+0&g0.tab=0">max(max_over_time(spark_AssetMetadataGenerator_driver_gi_metadatagenerator_success_total_count[3h])) == 0</a>
  FOR 1h
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;Metadata generator was not run successfully for the last three hours.&#34;, summary=&#34;Metadata generator was not run successfully&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_metadata_generator_no_new_assets_added%22%7D&g0.tab=0">jaguar_metadata_generator_no_new_assets_added</a>
  IF <a href="/graph?g0.expr=min%28min_over_time%28spark_AssetMetadataGenerator_driver_gi_metadatagenerator_generate_zero_new_metadata_total_count%5B1h%5D%29%29+%21%3D+0&g0.tab=0">min(min_over_time(spark_AssetMetadataGenerator_driver_gi_metadatagenerator_generate_zero_new_metadata_total_count[1h])) != 0</a>
  FOR 1h
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;Metadata generator did not add more assets.&#34;, summary=&#34;Metadata generator did not add more assets&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22jaguar_metadata_generator_delayed%22%7D&g0.tab=0">jaguar_metadata_generator_delayed</a>
  IF <a href="/graph?g0.expr=avg_over_time%28spark_SparkAssetMetadataGenerator_driver_StreamingMetrics_streaming_lastCompletedBatch_schedulingDelay%5B1m%5D%29+%3E+1000&g0.tab=0">avg_over_time(spark_SparkAssetMetadataGenerator_driver_StreamingMetrics_streaming_lastCompletedBatch_schedulingDelay[1m]) &gt; 1000</a>
  FOR 30m
  LABELS {project=&#34;jaguar&#34;}
  ANNOTATIONS {description=&#34;Metadata generator was delayed for the last 10 minutes.&#34;, summary=&#34;Metadata generator was delayed&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22system_mesos_master_down%22%7D&g0.tab=0">system_mesos_master_down</a>
  IF <a href="/graph?g0.expr=sum%28up%7Bjob%3D%22node-exporter%22%2Cmesos_role%3D%22master%22%7D%29+%3C+5&g0.tab=0">sum(up{job=&#34;node-exporter&#34;,mesos_role=&#34;master&#34;}) &lt; 5</a>
  FOR 1m
  LABELS {owner=&#34;system&#34;, severity=&#34;critical&#34;}
  ANNOTATIONS {description=&#34;One or more Mesos masters (out of 5) have been down for over 1m&#34;, summary=&#34;One or more Mesos masters are down&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22system_dcos_url_down%22%7D&g0.tab=0">system_dcos_url_down</a>
  IF <a href="/graph?g0.expr=%28probe_http_status_code%7Binstance%3D%22dcos.uswest.getty.im%22%7D+%21%3D+200%29&g0.tab=0">(probe_http_status_code{instance=&#34;dcos.uswest.getty.im&#34;} != 200)</a>
  FOR 3m
  LABELS {ci=&#34;DCOS&#34;, noc_alert=&#34;f&#34;, owner=&#34;system&#34;, selfservice=&#34;t&#34;, severity=&#34;critical&#34;, team=&#34;GroundControl&#34;, tsa=&#34;KB0013754&#34;}
  ANNOTATIONS {description=&#34;We received a non-200 response from dcos.uswest.getty.im for 3 minutes&#34;, summary=&#34;URL is down for dcos.uswest.getty.im&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22system_mesos_master_low_disk%22%7D&g0.tab=0">system_mesos_master_low_disk</a>
  IF <a href="/graph?g0.expr=%28node_filesystem_free%7Bmesos_role%3D%22master%22%2Cmountpoint%3D%22%2Frootfs%22%7D+%2F+node_filesystem_size%7Bmesos_role%3D%22master%22%2Cmountpoint%3D%22%2Frootfs%22%7D+%2A+100%29+%3C+20&g0.tab=0">(node_filesystem_free{mesos_role=&#34;master&#34;,mountpoint=&#34;/rootfs&#34;} / node_filesystem_size{mesos_role=&#34;master&#34;,mountpoint=&#34;/rootfs&#34;} * 100) &lt; 20</a>
  FOR 2h
  LABELS {owner=&#34;system&#34;, severity=&#34;critical&#34;}
  ANNOTATIONS {description=&#34;One or more Mesos masters has under 20% disk available on /&#34;, summary=&#34;Mesos Master disk alert&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22system_mesos_agent_low_disk_root%22%7D&g0.tab=0">system_mesos_agent_low_disk_root</a>
  IF <a href="/graph?g0.expr=%28node_filesystem_free%7Bmesos_role%3D%22agent%22%2Cmountpoint%3D%22%2Frootfs%22%7D+%2F+node_filesystem_size%7Bmesos_role%3D%22agent%22%2Cmountpoint%3D%22%2Frootfs%22%7D+%2A+100%29+%3C+10&g0.tab=0">(node_filesystem_free{mesos_role=&#34;agent&#34;,mountpoint=&#34;/rootfs&#34;} / node_filesystem_size{mesos_role=&#34;agent&#34;,mountpoint=&#34;/rootfs&#34;} * 100) &lt; 10</a>
  FOR 2h
  LABELS {mountpoint=&#34;/&#34;, owner=&#34;system&#34;, severity=&#34;critical&#34;}
  ANNOTATIONS {description=&#34;One or more Mesos agents has under 10% disk available on /&#34;, summary=&#34;Mesos Agent disk alert&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22system_mesos_agent_low_disk_var_lib%22%7D&g0.tab=0">system_mesos_agent_low_disk_var_lib</a>
  IF <a href="/graph?g0.expr=%28node_filesystem_free%7Bmesos_role%3D%22agent%22%2Cmountpoint%3D%22%2Frootfs%2Fvar%2Flib%22%7D+%2F+node_filesystem_size%7Bmesos_role%3D%22agent%22%2Cmountpoint%3D%22%2Frootfs%2Fvar%2Flib%22%7D+%2A+100%29+%3C+10&g0.tab=0">(node_filesystem_free{mesos_role=&#34;agent&#34;,mountpoint=&#34;/rootfs/var/lib&#34;} / node_filesystem_size{mesos_role=&#34;agent&#34;,mountpoint=&#34;/rootfs/var/lib&#34;} * 100) &lt; 10</a>
  FOR 2h
  LABELS {mountpoint=&#34;/var/lib&#34;, owner=&#34;system&#34;, severity=&#34;critical&#34;}
  ANNOTATIONS {description=&#34;One or more Mesos agents has under 10% disk available on /var/lib&#34;, summary=&#34;Mesos Agent disk alert&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22system_mesos_marathonlb_internal_not_running%22%7D&g0.tab=0">system_mesos_marathonlb_internal_not_running</a>
  IF <a href="/graph?g0.expr=%28absent%28marathon_app_instances%7Bapp%3D%22%2Fsystem%2Fmarathon-lb-internal%22%7D%29+or+sum%28marathon_app_task_running%7Bapp%3D%22%2Fsystem%2Fmarathon-lb-internal%22%7D%29+%3C+3%29&g0.tab=0">(absent(marathon_app_instances{app=&#34;/system/marathon-lb-internal&#34;}) or sum(marathon_app_task_running{app=&#34;/system/marathon-lb-internal&#34;}) &lt; 3)</a>
  FOR 5m
  LABELS {owner=&#34;system&#34;, severity=&#34;critical&#34;}
  ANNOTATIONS {description=&#34;One or more INTERNAL ingress load balancer instances (out of 3) are not running.&#34;, summary=&#34;Mesos ingress load balancer alert&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22system_mesos_marathonlb_external_not_running%22%7D&g0.tab=0">system_mesos_marathonlb_external_not_running</a>
  IF <a href="/graph?g0.expr=%28absent%28marathon_app_instances%7Bapp%3D%22%2Fsystem%2Fmarathon-lb-external%22%7D%29+or+sum%28marathon_app_task_running%7Bapp%3D%22%2Fsystem%2Fmarathon-lb-external%22%7D%29+%3C+3%29&g0.tab=0">(absent(marathon_app_instances{app=&#34;/system/marathon-lb-external&#34;}) or sum(marathon_app_task_running{app=&#34;/system/marathon-lb-external&#34;}) &lt; 3)</a>
  FOR 5m
  LABELS {owner=&#34;system&#34;, severity=&#34;critical&#34;}
  ANNOTATIONS {description=&#34;One or more EXTERNAL ingress load balancer instances (out of 3) are not running.&#34;, summary=&#34;Mesos ingress load balancer alert&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22flapping__marathon_app%22%7D&g0.tab=0">flapping__marathon_app</a>
  IF <a href="/graph?g0.expr=marathon_app_task_running+%3E+0+and+max_over_time%28marathon_app_task_avg_uptime%5B10m%5D%29+%3C+30&g0.tab=0">marathon_app_task_running &gt; 0 and max_over_time(marathon_app_task_avg_uptime[10m]) &lt; 30</a>
  FOR 10m
  LABELS {owner=&#34;system&#34;, severity=&#34;warning&#34;}
  ANNOTATIONS {description=&#34;A Service (Marathon app) has been flapping for at least 10 minutes&#34;, summary=&#34;Marathon App/Service alert&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22system_flapping_marathon%22%7D&g0.tab=0">system_flapping_marathon</a>
  IF <a href="/graph?g0.expr=marathon_service_mesosphere_marathon_uptime+%3C+60&g0.tab=0">marathon_service_mesosphere_marathon_uptime &lt; 60</a>
  FOR 5m
  LABELS {owner=&#34;system&#34;, severity=&#34;critical&#34;}
  ANNOTATIONS {description=&#34;Marathon has been flapping for at least 10 minutes&#34;, summary=&#34;Marathon alert&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22system_flapping_mesos_master%22%7D&g0.tab=0">system_flapping_mesos_master</a>
  IF <a href="/graph?g0.expr=mesos_master_uptime_seconds+%3C+300&g0.tab=0">mesos_master_uptime_seconds &lt; 300</a>
  FOR 15m
  LABELS {owner=&#34;system&#34;, severity=&#34;critical&#34;}
  ANNOTATIONS {description=&#34;A Mesos master has been flapping for at least 15 minutes&#34;, summary=&#34;Mesos alert&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22system_flapping_mesos_agent%22%7D&g0.tab=0">system_flapping_mesos_agent</a>
  IF <a href="/graph?g0.expr=mesos_slave_uptime_seconds+%3C+300&g0.tab=0">mesos_slave_uptime_seconds &lt; 300</a>
  FOR 15m
  LABELS {owner=&#34;system&#34;, severity=&#34;critical&#34;}
  ANNOTATIONS {description=&#34;A Mesos agent has been flapping for at least 15 minutes&#34;, summary=&#34;Mesos alert&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22BlackboxExporterDown%22%7D&g0.tab=0">BlackboxExporterDown</a>
  IF <a href="/graph?g0.expr=count%28up%7Bjob%3D%22blackbox%22%7D+%3D%3D+1%29+%3C+1&g0.tab=0">count(up{job=&#34;blackbox&#34;} == 1) &lt; 1</a>
  FOR 3m
  LABELS {ci=&#34;DCOS&#34;, noc_alert=&#34;f&#34;, owner=&#34;system&#34;, selfservice=&#34;t&#34;, severity=&#34;critical&#34;, team=&#34;GroundControl&#34;, tsa=&#34;KB0013754&#34;}
  ANNOTATIONS {description=&#34;The blackbox exporter (which provides other critical alerts) has been down for at least 3 minutes&#34;, summary=&#34;Blackbox exporter alert&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22container_failure_threshold_exceeded%22%7D&g0.tab=0">container_failure_threshold_exceeded</a>
  IF <a href="/graph?g0.expr=%28sum%28increase%28mesos_slave_task_states_exit_total%7Bstate%21~%22killed%7Cfinished%22%7D%5B5m%5D%29%29+%2F+sum%28mesos_slave_task_states_current%29+%3E+0.9%29&g0.tab=0">(sum(increase(mesos_slave_task_states_exit_total{state!~&#34;killed|finished&#34;}[5m])) / sum(mesos_slave_task_states_current) &gt; 0.9)</a>
  FOR 5m
  LABELS {owner=&#34;system&#34;, severity=&#34;warning&#34;}
  ANNOTATIONS {description=&#34;Too many containers are failing&#34;, summary=&#34;The Container Failure Rate is too damn high&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22system_mesos_critical_url_down%22%7D&g0.tab=0">system_mesos_critical_url_down</a>
  IF <a href="/graph?g0.expr=%28probe_http_status_code+%21%3D+200%29&g0.tab=0">(probe_http_status_code != 200)</a>
  FOR 1m
  LABELS {ci=&#34;DCOS&#34;, noc_alert=&#34;f&#34;, owner=&#34;system&#34;, selfservice=&#34;t&#34;, severity=&#34;critical&#34;, team=&#34;GroundControl&#34;, tsa=&#34;KB0013754&#34;}
  ANNOTATIONS {description=&#34;We received a non-200 response from {{ $labels.instance }} for 5 minutes&#34;, summary=&#34;URL is down for {{ $labels.instance }}&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22system_low_cpu_in_rack%22%7D&g0.tab=0">system_low_cpu_in_rack</a>
  IF <a href="/graph?g0.expr=sum%28mesos_slave_cpus%7Bingress%3D%22%22%2Cjob%3D%22mesos_exporter_agents%22%2Crack%21%3D%22%22%2Ctype%3D%22used%22%7D%29+BY+%28rack%29+%3E+%28sum%28mesos_slave_cpus%7Bingress%3D%22%22%2Cjob%3D%22mesos_exporter_agents%22%2Crack%21%3D%22%22%7D%29+BY+%28rack%29+%2A+0.9%29&g0.tab=0">sum(mesos_slave_cpus{ingress=&#34;&#34;,job=&#34;mesos_exporter_agents&#34;,rack!=&#34;&#34;,type=&#34;used&#34;}) BY (rack) &gt; (sum(mesos_slave_cpus{ingress=&#34;&#34;,job=&#34;mesos_exporter_agents&#34;,rack!=&#34;&#34;}) BY (rack) * 0.9)</a>
  FOR 30m
  LABELS {owner=&#34;system&#34;, severity=&#34;critical&#34;}
  ANNOTATIONS {description=&#34;Less than 10% CPU available in rack {{ $labels.rack }}&#34;, summary=&#34;Approaching maximum CPU usage in rack {{ $labels.rack }}&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22system_low_cpu_in_az%22%7D&g0.tab=0">system_low_cpu_in_az</a>
  IF <a href="/graph?g0.expr=sum%28mesos_slave_cpus%7Baz%21%3D%22%22%2Cingress%3D%22%22%2Cjob%3D%22mesos_exporter_agents%22%2Ctype%3D%22used%22%7D%29+BY+%28az%29+%3E+%28sum%28mesos_slave_cpus%7Baz%21%3D%22%22%2Cingress%3D%22%22%2Cjob%3D%22mesos_exporter_agents%22%7D%29+BY+%28az%29+%2A+0.9%29&g0.tab=0">sum(mesos_slave_cpus{az!=&#34;&#34;,ingress=&#34;&#34;,job=&#34;mesos_exporter_agents&#34;,type=&#34;used&#34;}) BY (az) &gt; (sum(mesos_slave_cpus{az!=&#34;&#34;,ingress=&#34;&#34;,job=&#34;mesos_exporter_agents&#34;}) BY (az) * 0.9)</a>
  FOR 30m
  LABELS {owner=&#34;system&#34;, severity=&#34;critical&#34;}
  ANNOTATIONS {description=&#34;Less than 10% CPU available in az {{ $labels.az }}&#34;, summary=&#34;Approaching maximum CPU usage in az {{ $labels.az }}&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22system_low_mem_in_rack%22%7D&g0.tab=0">system_low_mem_in_rack</a>
  IF <a href="/graph?g0.expr=sum%28mesos_slave_mem%7Bingress%3D%22%22%2Cjob%3D%22mesos_exporter_agents%22%2Crack%21%3D%22%22%2Ctype%3D%22used%22%7D%29+BY+%28rack%29+%3E+%28sum%28mesos_slave_mem%7Bingress%3D%22%22%2Cjob%3D%22mesos_exporter_agents%22%2Crack%21%3D%22%22%7D%29+BY+%28rack%29+%2A+0.9%29&g0.tab=0">sum(mesos_slave_mem{ingress=&#34;&#34;,job=&#34;mesos_exporter_agents&#34;,rack!=&#34;&#34;,type=&#34;used&#34;}) BY (rack) &gt; (sum(mesos_slave_mem{ingress=&#34;&#34;,job=&#34;mesos_exporter_agents&#34;,rack!=&#34;&#34;}) BY (rack) * 0.9)</a>
  FOR 30m
  LABELS {owner=&#34;system&#34;, severity=&#34;critical&#34;}
  ANNOTATIONS {description=&#34;Less than 10% memory available in rack {{ $labels.rack }}&#34;, summary=&#34;Approaching maximum memory usage in rack {{ $labels.rack }}&#34;}<br/>ALERT <a href="/graph?g0.expr=ALERTS%7Balertname%3D%22system_low_mem_in_az%22%7D&g0.tab=0">system_low_mem_in_az</a>
  IF <a href="/graph?g0.expr=sum%28mesos_slave_mem%7Baz%21%3D%22%22%2Cingress%3D%22%22%2Cjob%3D%22mesos_exporter_agents%22%2Ctype%3D%22used%22%7D%29+BY+%28az%29+%3E+%28sum%28mesos_slave_mem%7Baz%21%3D%22%22%2Cingress%3D%22%22%2Cjob%3D%22mesos_exporter_agents%22%7D%29+BY+%28az%29+%2A+0.9%29&g0.tab=0">sum(mesos_slave_mem{az!=&#34;&#34;,ingress=&#34;&#34;,job=&#34;mesos_exporter_agents&#34;,type=&#34;used&#34;}) BY (az) &gt; (sum(mesos_slave_mem{az!=&#34;&#34;,ingress=&#34;&#34;,job=&#34;mesos_exporter_agents&#34;}) BY (az) * 0.9)</a>
  FOR 30m
  LABELS {owner=&#34;system&#34;, severity=&#34;critical&#34;}
  ANNOTATIONS {description=&#34;Less than 10% memory available in az {{ $labels.az }}&#34;, summary=&#34;Approaching maximum memory usage in az {{ $labels.az }}&#34;}<br/></pre>
  </div>

  </body>
</html>



